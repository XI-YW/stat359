{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c7fcefa2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Assignment 3: Sentiment Classification Reflection\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    self-contained: true\n",
    "    number-sections: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a9ef7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9b22828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>test_macro_f1</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.800104</td>\n",
       "      <td>0.814305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.823934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.743383</td>\n",
       "      <td>0.773040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRU</td>\n",
       "      <td>0.740908</td>\n",
       "      <td>0.771664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.692936</td>\n",
       "      <td>0.726272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RNN</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.719395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  test_macro_f1  test_accuracy\n",
       "4  BERT       0.800104       0.814305\n",
       "5   GPT       0.798883       0.823934\n",
       "1  LSTM       0.743383       0.773040\n",
       "3   GRU       0.740908       0.771664\n",
       "0   MLP       0.692936       0.726272\n",
       "2   RNN       0.686558       0.719395"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_metrics_txt(path):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    d = {}\n",
    "    for line in path.read_text(encoding=\"utf-8\").splitlines():\n",
    "        if not line.strip() or \"\\t\" not in line:\n",
    "            continue\n",
    "        k, v = line.split(\"\\t\", 1)\n",
    "        try:\n",
    "            d[k.strip()] = float(v.strip())\n",
    "        except ValueError:\n",
    "            d[k.strip()] = v.strip()\n",
    "    return d\n",
    "\n",
    "models = {\n",
    "    \"MLP\": \"mlp\",\n",
    "    \"LSTM\": \"lstm\",\n",
    "    \"RNN\": \"rnn\",\n",
    "    \"GRU\": \"gru\",\n",
    "    \"BERT\": \"bert\",\n",
    "    \"GPT\": \"gpt\",\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, prefix in models.items():\n",
    "    m = read_metrics_txt(OUT / f\"{prefix}_metrics.txt\")\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"test_macro_f1\": None if m is None else m.get(\"test_macro_f1\"),\n",
    "        \"test_accuracy\": None if m is None else m.get(\"test_accuracy\"),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df_sorted = df.sort_values(by=\"test_macro_f1\", ascending=False, na_position=\"last\")\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc579691",
   "metadata": {},
   "source": [
    "## Plots generated by the training scripts\n",
    "\n",
    "### MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33f797",
   "metadata": {},
   "source": [
    "![MLP Learning Curves](outputs/mlp_learning_curves.png)\n",
    "![MLP Confusion Matrix](outputs/mlp_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1a8d0",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c97a54",
   "metadata": {},
   "source": [
    "![LSTM Learning Curves](outputs/lstm_learning_curves.png)\n",
    "![LSTM Confusion Matrix](outputs/lstm_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd527659",
   "metadata": {},
   "source": [
    "### **1. Training Dynamics**\n",
    "Focus on your MLP and LSTM implementations\n",
    "\n",
    "#### **Did your models show signs of overfitting or underfitting? What architectural or training changes could address this?**\n",
    "\n",
    "- MLP:\n",
    "    - The training loss keeps declining, but the validation loss stops improving early and then keeps relatively stable. \n",
    "    - In both macro f1 and accuracy curves, there are small performance gaps between the training set and the validation set.\n",
    "    Thus, we can say that the MLP model shows a sign of slight overfitting, but the situation seems to be acceptable.\n",
    "\n",
    "- LSTM:\n",
    "    - The training loss keeps declining, but the validation loss stops improving early and then starts to drift upward.\n",
    "    - In both macro f1 and accuracy curves, there are large performance gaps between the training set and the validation set.\n",
    "    This implies that the LSTM model shows a sign of severe overfitting. This likely occurs because mildly positive financial statements often resemble neutral ones in tone, making the boundary between Positive and Neutral difficult to distinguish. As a result, many positive examples are predicted as neutral, increasing the error rate for the Positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33138137",
   "metadata": {},
   "source": [
    "#### **How did using class weights affect training stability and final performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b8533",
   "metadata": {},
   "source": [
    "The dataset is imbalanced with neutral as the majority class. Without class weights, optimization tends to favor the majority class, which can inflate accuracy but hurt macro f1 score since minority classes get low recall. Using class weights changes the gradient contribution so minority class mistakes are penalized more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fbbc64",
   "metadata": {},
   "source": [
    "### **2. Model Performance and Error Analysis**\n",
    "Focus on your MLP and LSTM implementations\n",
    "\n",
    "#### **Which of your two models generalized better to the test set? Provide evidence from your metrics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e407ea89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>test_macro_f1</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.800104</td>\n",
       "      <td>0.814305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.823934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.743383</td>\n",
       "      <td>0.773040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRU</td>\n",
       "      <td>0.740908</td>\n",
       "      <td>0.771664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.692936</td>\n",
       "      <td>0.726272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RNN</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.719395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  test_macro_f1  test_accuracy\n",
       "4  BERT       0.800104       0.814305\n",
       "5   GPT       0.798883       0.823934\n",
       "1  LSTM       0.743383       0.773040\n",
       "3   GRU       0.740908       0.771664\n",
       "0   MLP       0.692936       0.726272\n",
       "2   RNN       0.686558       0.719395"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa4b27",
   "metadata": {},
   "source": [
    "Observe that the LSTM model has higher macro F1 score and test accuracy. Thus, the LSTM model generalized better to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea608b5",
   "metadata": {},
   "source": [
    "#### **Which sentiment class was most frequently misclassified? Propose reasons for this pattern.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c09eea",
   "metadata": {},
   "source": [
    "We focus on the confusion matrix:\n",
    "![MLP Confusion Matrix](outputs/mlp_confusion_matrix.png)\n",
    "![LSTM Confusion Matrix](outputs/lstm_confusion_matrix.png)\n",
    "\n",
    "\n",
    "- MLP (number of misclassifications)\n",
    "    - Negative: 17 (18.7%)\n",
    "    - Neutral: 108 (25.0%)\n",
    "    - Postive: 74 (36.3%)\n",
    "\n",
    "- LSTM (number of misclassifications)\n",
    "    - Negative: 16 (17.6%)\n",
    "    - Neutral: 76 (17.6%)\n",
    "    - Postive: 73 (35.8%)\n",
    "\n",
    "In both models, the neutral class had the largest number of misclassifications, which is probably because of its large total number.\n",
    "\n",
    "In both models, the positive class had the highest misclassification rate. This likely occurs because mildly positive financial statements often resemble neutral ones in tone, making the boundary between Positive and Neutral difficult to distinguish. As a result, many positive examples are predicted as neutral, increasing the error rate for the Positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5f771",
   "metadata": {},
   "source": [
    "### **3. Cross-Model Comparison**\n",
    "Compare all six models: MLP, RNN, LSTM, GRU, BERT, GPT\n",
    "\n",
    "#### **How did mean-pooled FastText embeddings limit the MLP compared to sequence-based models?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71372136",
   "metadata": {},
   "source": [
    "Mean-pooled FastText embeddings limit the MLP because averaging word vectors removes word order and structural information from the sentence. This makes the model unable to properly capture negation, phrase-level interactions, or compositional meaning. In contrast, sequence-based models like LSTM process tokens in order and can model contextual dependencies, resulting in richer sentence representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a16cd",
   "metadata": {},
   "source": [
    "#### **What advantage did the LSTM’s sequential processing provide over the MLP?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27138b2",
   "metadata": {},
   "source": [
    "The LSTM’s sequential processing allows it to model word order and contextual dependencies within a sentence, while the MLP relies on mean-pooled embeddings and ignores order. This enables the LSTM to better capture negation, modifiers, and multi-word expressions that influence sentiment. As a result, the LSTM can learn richer sentence representations, although its higher capacity also increases the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1aaa9",
   "metadata": {},
   "source": [
    "#### **Did fine-tuned LLMs (BERT/GPT) outperform classical baselines? Explain the performance gap in terms of pretraining and contextual representations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b0667",
   "metadata": {},
   "source": [
    "Yes，according to the metrics, BERT and GPT have test macro f1 scores and test accuracies that are significantly higher than the other four models, implying their outstanding performance.\n",
    "\n",
    "Fine‑tuned BERT/GPT outperform classical baselines because they start from pretrained contextual representations:\n",
    "- Pretraining on massive corpora teaches general syntax.\n",
    "- Unlike static FastText, contextual embeddings represent the meaning of a token conditioned on surrounding tokens.\n",
    "- Self‑attention captures long range dependencies without the limitations of vanilla recurrence.\n",
    "\n",
    "Even with a small labeled dataset, fine‑tuning can leverage this prior knowledge to improve macro f1 and reduce systematic class confusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb50012",
   "metadata": {},
   "source": [
    "#### **Rank all six models by test performance. What architectural or representational factors explain the ranking?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a12dbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>test_macro_f1</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.800104</td>\n",
       "      <td>0.814305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.823934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.743383</td>\n",
       "      <td>0.773040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRU</td>\n",
       "      <td>0.740908</td>\n",
       "      <td>0.771664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.692936</td>\n",
       "      <td>0.726272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RNN</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.719395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  test_macro_f1  test_accuracy\n",
       "4  BERT       0.800104       0.814305\n",
       "5   GPT       0.798883       0.823934\n",
       "1  LSTM       0.743383       0.773040\n",
       "3   GRU       0.740908       0.771664\n",
       "0   MLP       0.692936       0.726272\n",
       "2   RNN       0.686558       0.719395"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831b9cf",
   "metadata": {},
   "source": [
    "We use the test macro f1 score to rank the models, since the dataset is imbalanced and Macro-F1 better reflects balanced performance across all sentiment classes.\n",
    "The final ranking is that: BERT > GPT > LSTM > GRU > MLP > RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Use Disclosure (Required)\n",
    "\n",
    "If you used any AI-enabled tools (e.g., ChatGPT, GitHub Copilot, Claude, or other LLM assistants) while working on this assignment, you must disclose that use here. The goal is transparency-not punishment.\n",
    "\n",
    "In your disclosure, briefly include:\n",
    "- **Tool(s) used:** (name + version if known)\n",
    "- **How you used them:** (e.g., concept explanation, debugging, drafting code, rewriting text)\n",
    "- **What you verified yourself:** (e.g., reran the notebook, checked outputs/plots, checked shapes, read documentation)\n",
    "- **What you did *not* use AI for (if applicable):** (optional)\n",
    "\n",
    "You are responsible for the correctness of your submission, even if AI suggested code or explanations.\n",
    "\n",
    "#### <font color=\"red\">Write your disclosure here.</font>\n",
    "\n",
    "- **Tool(s) used:** Google Gemini\n",
    "- **How you used them:** concept explanation, debugging, drafting code, checking the validity of the result\n",
    "- **What you verified yourself:** checked outputs/plots, checked shapes\n",
    "- **What you did *not* use AI for (if applicable):** drafted the notebook, adjusted the format of the notebook, did analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d37e4b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat359-su25-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
